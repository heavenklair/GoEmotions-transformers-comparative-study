{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# from modules.lstm_encoder import LSTMEncoder\n",
    "\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: go_emotions/simplified\n",
      "Found cached dataset go_emotions (/Users/heaven/.cache/huggingface/datasets/go_emotions/simplified/0.0.0/2637cfdd4e64d30249c3ed2150fa2b9d279766bfcd6a809b9f085c61a90d776d)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f7a6162d324452bae02302c11ffc8be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset_name = \"go_emotions\"\n",
    "dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels', 'id'],\n",
       "        num_rows: 43410\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'labels', 'id'],\n",
       "        num_rows: 5426\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels', 'id'],\n",
       "        num_rows: 5427\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract train, dev, and test sets\n",
    "X_train, y_train = dataset['train']['text'], dataset['train']['labels']\n",
    "X_dev, y_dev = dataset['validation']['text'], dataset['validation']['labels']\n",
    "X_test, y_test = dataset['test']['text'], dataset['test']['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# X_train = [x.lower() for x in X_train]\n",
    "\n",
    "# X_train = [''.join(ch for ch in x if ch not in string.punctuation) for x in X_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating embedding using Glove\n",
    "\n",
    "1. Load GloVe Embeddings\n",
    "2. Build Vocabulary from Your Dataset\n",
    "3. Create a Mapping of Words to Unique IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveTokenizer:\n",
    "    def __init__(self, glove_file_path):\n",
    "        self.word2id = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "        self.id2word = {0: \"<pad>\", 1: \"<unk>\"}\n",
    "        self.word2vec = {}\n",
    "        self.embeddings = []\n",
    "        \n",
    "        # Load GloVe vectors\n",
    "        with open(glove_file_path, 'r', encoding='utf8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype=np.float32)\n",
    "                self.word2vec[word] = vector\n",
    "        \n",
    "        # Prepopulate embeddings with pad and unk tokens\n",
    "        embedding_dim = len(next(iter(self.word2vec.values())))\n",
    "        self.embeddings.append(np.zeros(embedding_dim))\n",
    "        self.embeddings.append(np.random.rand(embedding_dim))\n",
    "        \n",
    "    def build_vocab(self, data, vocab_size):\n",
    "        # Count words\n",
    "        word_counts = Counter()\n",
    "        for text in data:\n",
    "            tokens = text.split()\n",
    "            word_counts.update(tokens)\n",
    "        \n",
    "        # Sort by frequency and take top vocab_size words\n",
    "        for word, _ in word_counts.most_common(vocab_size - len(self.word2id)):\n",
    "            if word not in self.word2id:\n",
    "                self.word2id[word] = len(self.word2id)\n",
    "                self.id2word[self.word2id[word]] = word\n",
    "                if word in self.word2vec:\n",
    "                    self.embeddings.append(self.word2vec[word])\n",
    "                else:\n",
    "                    self.embeddings.append(np.random.rand(embedding_dim))\n",
    "                    \n",
    "        self.embeddings = np.array(self.embeddings)\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        return [self.word2id.get(word, self.word2id[\"<unk>\"]) for word in text.split()]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return \" \".join([self.id2word[token] for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file_path = \"path_to_glove_vectors.txt\"\n",
    "tokenizer = GloveTokenizer(glove_file_path)\n",
    "tokenizer.build_vocab(X_train, vocab_size=50000)\n",
    "\n",
    "# To tokenize\n",
    "X_train_tokenized = [tokenizer.tokenize(text) for text in X_train]\n",
    "X_dev_tokenized = [tokenizer.tokenize(text) for text in X_dev]\n",
    "X_test_tokenized = [tokenizer.tokenize(text) for text in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To decode\n",
    "decoded_text = tokenizer.decode(tokenized_train[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
